{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVgMUio8CRvN562K2cXqPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seschm/Internship-Gaertner/blob/main/Reproduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hN3kjruCQdy"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, vmap, pmap, grad\n",
        "from jax import random\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import torch.utils.data as data\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import random as rng\n",
        "\n",
        "import numpy as np\n",
        "from numpy import sqrt, cos, sin, exp, pi, log2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import unitary_group\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_pure_state(nQubit):\n",
        "    \"\"\"\n",
        "    Generates Haar random pure state.\n",
        "    To generate a random pure state, take any basis state, e.g. |00...00>\n",
        "    and apply a random unitary matrix. For consistency each basis state should be the same. \n",
        "    \"\"\"\n",
        "    baseRho=np.zeros((2**nQubit,2**nQubit),dtype=complex)\n",
        "    baseRho[0,0]=1\n",
        "    U=unitary_group.rvs(2**nQubit)\n",
        "    return U@baseRho@U.conj().T"
      ],
      "metadata": {
        "id": "LIyvy87aCXSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_single_qubit(theta,phi):\n",
        "    \"\"\"\n",
        "    Generates single qubit out of the given angles theta and phi.\n",
        "    First construct the single qubit state as an array of shape (2, 1).\n",
        "    Then compute the matrixproduct with its adjoint state.\n",
        "    \"\"\"\n",
        "    state = np.array([[cos(theta/2)],[sin(theta/2)*exp(phi*1.j)]])\n",
        "    return state@state.conj().T"
      ],
      "metadata": {
        "id": "p58FzmPrCaJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_seperable_pure_state(nQubit):\n",
        "    \"\"\"\n",
        "    Generates random seperable pure state.\n",
        "    First generate the desired number of random pure states.\n",
        "    Then tensor them together.\n",
        "    \"\"\"\n",
        "    single_qubits = []\n",
        "    for i in range(0,nQubit):\n",
        "        single_qubits.append(generate_random_pure_state(1))\n",
        "    tensored_qubits = [single_qubits[0]]\n",
        "    for i in range(1,nQubit):\n",
        "        tensored_qubits.append(np.kron(tensored_qubits[-1],single_qubits[i]))    \n",
        "    return tensored_qubits[-1]"
      ],
      "metadata": {
        "id": "HBf7B1hiZU82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def depolarizing_channel(state,p):\n",
        "    \"\"\"\n",
        "    Applies a depolarizing channel to the given state with p the probability of the completely mixed state.\n",
        "    \"\"\"\n",
        "    d=len(state[0])\n",
        "    return p*np.eye(d)/d+(1-p)*state"
      ],
      "metadata": {
        "id": "6NtxWOP_Cinx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_POVM(theta,phi,nQubit):\n",
        "    \"\"\"\n",
        "    Generates a POVM consisting of multi qubit projectors on to the axis defined by the two angles theta and phi.\n",
        "    First generates all possible states (combinations of spin up and down) as strings containing 1s and 0s.\n",
        "    The order is equal to binary counting.\n",
        "    Then generates the single qubit projector and its orthogonal projector.\n",
        "    In the next step all single qubit projectors corresponding to the same state are gathered in the correct order and then tensored together to get an element of the POVM.\n",
        "    Returns the POVM with the elements beeing ordered equal to binary counting (-> up ... up, up ... up down, up ... up down up, ...).\n",
        "    \"\"\"\n",
        "    up_and_downs = []\n",
        "\n",
        "    for i in range(2**nQubit):\n",
        "        binary = bin(i)[2:]\n",
        "        zeros = np.zeros(nQubit-len(binary), dtype=int)\n",
        "        for k in range(nQubit-len(binary)):\n",
        "            binary = '0' + binary\n",
        "        up_and_downs.append(binary)\n",
        "\n",
        "    projector=generate_single_qubit(theta,phi)\n",
        "    orthogonal_projector = np.eye(2)-projector\n",
        "    POVM = []\n",
        "\n",
        "    for i in range(2**nQubit):\n",
        "        tensored_projector = []\n",
        "        single_projectors = []\n",
        "        for j in range(nQubit):\n",
        "            if up_and_downs[i][j] == '0':\n",
        "                single_projectors.append(projector)\n",
        "            if up_and_downs[i][j] == '1':\n",
        "                single_projectors.append(orthogonal_projector)\n",
        "        tensored_projector.append(single_projectors[0])\n",
        "        for k in range(nQubit-1):\n",
        "            tensored_projector.append(np.kron(tensored_projector[-1],single_projectors[k+1]))\n",
        "        POVM.append(tensored_projector[-1])\n",
        "    \n",
        "    return POVM"
      ],
      "metadata": {
        "id": "2k7feg9FClHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_multi_qubit_measurement(state,POVM):\n",
        "    \"\"\"\n",
        "    Performs a multi qubit measurement with the given POVM.\n",
        "    \"\"\"\n",
        "    probabilities = []\n",
        "    for element in POVM:\n",
        "        probabilities.append(np.trace(element@state))\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "SE8lbqzYCnde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_noisy_multi_qubit_measurement(state,POVM,p):\n",
        "    \"\"\"\n",
        "    Generates probabilities of the noisy state.\n",
        "    First applies the noise, then generates the probabilities.\n",
        "    \"\"\"\n",
        "    noisy_state = depolarizing_channel(state,p)\n",
        "    return perform_multi_qubit_measurement(noisy_state,POVM)"
      ],
      "metadata": {
        "id": "aR_3HZAyCpb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(probabilities,samplesize=1000):\n",
        "    \"\"\"\n",
        "    Takes in a probability distribution and samples from it.\n",
        "    \"\"\"\n",
        "    sampling = rng.choices(np.arange(0,len(probabilities)), weights=probabilities, k=samplesize)\n",
        "    sampled_probabilities = []\n",
        "    for element in np.arange(0,len(probabilities)):\n",
        "        sampled_probabilities.append(sampling.count(element)/samplesize)\n",
        "    return sampled_probabilities"
      ],
      "metadata": {
        "id": "lWv3n9RvCrL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(state, params, batch_input, batch_target_output):\n",
        "    predictions = state.apply_fn(params, batch_input)\n",
        "    #loss = jnp.mean(optax.softmax_cross_entropy(predictions,batch_target_output))\n",
        "    loss = jnp.mean(-jnp.sum(batch_target_output*jax.lax.log(predictions),axis=1))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dQawMgfKz7Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def accuracy_MSE(acc_input, acc_target_output,nQubit):\n",
        "  acc = jnp.mean(jnp.sum(jnp.abs(acc_input-acc_target_output)**2,axis=2)/(2**nQubit))\n",
        "  return acc"
      ],
      "metadata": {
        "id": "-yc7JW_aDXAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def accuracy_KLD(acc_input, acc_target_output):\n",
        "  acc = jnp.mean(jnp.sum(acc_target_output*jnp.log(acc_target_output/acc_input),axis=2))\n",
        "  return acc"
      ],
      "metadata": {
        "id": "dTWh4ulU1f3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def accuracy_IF(acc_input, acc_target_output):\n",
        "  acc = jnp.mean(1-(jnp.sum(jnp.sqrt(acc_target_output*acc_input),axis=2))**2)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "G4aaspSi1jqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit  # Jit the function for efficiency\n",
        "def train_step(state, batch_input, batch_target_output):\n",
        "    # Gradient function\n",
        "    grad_fn = jax.value_and_grad(loss_fn,# Function to calculate the loss\n",
        "                                 argnums=1,  # Parameters are second argument of the function\n",
        "                                )\n",
        "    # Determine gradients for current model, parameters and batch\n",
        "    loss, grads = grad_fn(state, state.params, batch_input, batch_target_output)\n",
        "    # Perform parameter update with gradients and optimizer\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    # Return state and any other value we might want\n",
        "    return state, loss"
      ],
      "metadata": {
        "id": "59_QFIoCEUQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(state, batched_training_input, batched_training_target_outputs, acc_input, acc_target_output,nQubit, num_epochs = 100):\n",
        "  \n",
        "  acc_test_MSE = []\n",
        "  acc_training_MSE = []\n",
        "  acc_unmitigated_test_MSE = []\n",
        "\n",
        "  acc_test_KLD = []\n",
        "  acc_training_KLD = []\n",
        "  acc_unmitigated_test_KLD = []\n",
        "\n",
        "  acc_test_IF = []\n",
        "  acc_training_IF = []\n",
        "  acc_unmitigated_test_IF = []\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    for i in range(len(batched_training_input)):\n",
        "      state, loss = train_step(state, batched_training_input[i], batched_training_target_outputs[i])\n",
        "\n",
        "    predictions_test = state.apply_fn(state.params, acc_input)\n",
        "    predictions_training = state.apply_fn(state.params, batched_training_input)\n",
        "\n",
        "    acc_test_MSE.append(accuracy_MSE(predictions_test, acc_target_output,nQubit))\n",
        "    acc_training_MSE.append(accuracy_MSE(predictions_training, batched_training_target_outputs,nQubit))\n",
        "    acc_unmitigated_test_MSE.append(accuracy_MSE(acc_input, acc_target_output,nQubit))\n",
        "\n",
        "    acc_test_KLD.append(accuracy_KLD(predictions_test, acc_target_output))\n",
        "    acc_training_KLD.append(accuracy_KLD(predictions_training, batched_training_target_outputs))\n",
        "    acc_unmitigated_test_KLD.append(accuracy_KLD(acc_input, acc_target_output))\n",
        "\n",
        "    acc_test_IF.append(accuracy_IF(predictions_test, acc_target_output))\n",
        "    acc_training_IF.append(accuracy_IF(predictions_training, batched_training_target_outputs))\n",
        "    acc_unmitigated_test_IF.append(accuracy_IF(acc_input, acc_target_output))\n",
        "\n",
        "  return state, [acc_test_MSE, acc_training_MSE, acc_unmitigated_test_MSE], [acc_test_KLD, acc_training_KLD, acc_unmitigated_test_KLD], [acc_test_IF, acc_training_IF, acc_unmitigated_test_IF]"
      ],
      "metadata": {
        "id": "k2fXIYDkEXop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  num_neurons_per_layer : list[int]   # List containing the number of neurons per layer (except input layer)\n",
        "\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    activation = x\n",
        "    for i, numb_neurons in enumerate(self.num_neurons_per_layer):\n",
        "      activation = nn.Dense(numb_neurons)(activation) # tell how to initialize weigths -> reduce variance\n",
        "      if i != len(self.num_neurons_per_layer) - 1:\n",
        "        activation = nn.relu(activation)\n",
        "    return jax.nn.softmax(activation)"
      ],
      "metadata": {
        "id": "BgkYgZmrC5hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nQubit = 2\n",
        "layers = [20,20,20,20,20,2**nQubit]\n",
        "scale = 1 # scale the variance of the initial params\n",
        "num_epochs = 300\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "training_states = 1800\n",
        "test_states = 200\n",
        "training_data = np.zeros((training_states,2**nQubit*2))\n",
        "test_data = np.zeros((test_states,2**nQubit*2))\n",
        "theta = 0\n",
        "phi = 0\n",
        "p = 0.15"
      ],
      "metadata": {
        "id": "kvt_Vb1UE7tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(num_neurons_per_layer=layers)        # Initialize network\n",
        "x_key, init_key = random.split(random.PRNGKey(0))\n",
        "x = scale*random.normal(x_key, (batch_size, 2**nQubit))                    # Batch size 100, input size 2\n",
        "params = model.init(init_key, x)                      # Initialize network params (biases being initialized as zeros)"
      ],
      "metadata": {
        "id": "3UD_DWJsFuTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optax.adam(learning_rate=learning_rate) #sgd\n",
        "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
        "                                            params=params,\n",
        "                                            tx=optimizer)"
      ],
      "metadata": {
        "id": "QS0Gv73WFwjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POVM = generate_POVM(theta,phi,nQubit)\n",
        "\n",
        "for i in tqdm(range(training_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #training_data[i][j] = noisy_probs_sampled[j]\n",
        "        training_data[i][j] = noisy_probs[j]\n",
        "        training_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "for i in tqdm(range(test_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #test_data[i][j] = noisy_probs_sampled[j]\n",
        "        test_data[i][j] = noisy_probs[j]\n",
        "        test_data[i][j+2**nQubit] = probs[j]"
      ],
      "metadata": {
        "id": "CnOJIj-CFdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "\n",
        "data_loader_training = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_training_input = []  \n",
        "batched_training_target_outputs = []\n",
        "\n",
        "for batch in data_loader_training:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_training_input.append(inputs)\n",
        "  batched_training_target_outputs.append(targets)\n",
        "\n",
        "\n",
        "data_loader_test = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_test_input = []  \n",
        "batched_test_target_outputs = []\n",
        "\n",
        "for batch in data_loader_test:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_test_input.append(inputs)\n",
        "  batched_test_target_outputs.append(targets)\n",
        "\n",
        "batched_tr_inp = jnp.asarray(batched_training_input)\n",
        "batched_tr_tar_out = jnp.asarray(batched_training_target_outputs)\n",
        "batched_test_inp = jnp.asarray(batched_test_input)\n",
        "batched_test_tar_out = jnp.asarray(batched_test_target_outputs)"
      ],
      "metadata": {
        "id": "QW6iKAVMFlh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_state, MSE_data, KLD_data, IF_data = train_model(model_state, batched_tr_inp, batched_tr_tar_out, \n",
        "                                                               batched_test_inp, batched_test_tar_out, \n",
        "                                                               nQubit=nQubit, num_epochs = num_epochs)"
      ],
      "metadata": {
        "id": "lNJqed4CF4th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.arange(0,num_epochs)\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "axs[0].plot(i,MSE_data[0],label=\"MSE test data\",color=\"red\")\n",
        "axs[0].plot(i,MSE_data[1],label=\"MSE training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[0].plot(i,MSE_data[2],label=\"MSE unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[0].set_xlabel('Number of epochs')\n",
        "axs[0].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[0].set_yscale('log')\n",
        "axs[0].legend()\n",
        "axs[1].plot(i,KLD_data[0],label=\"KLD test data\",color=\"red\")\n",
        "axs[1].plot(i,KLD_data[1],label=\"KLD training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[1].plot(i,KLD_data[2],label=\"KLD unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[1].set_xlabel('Number of epochs')\n",
        "axs[1].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[1].set_yscale('log')\n",
        "axs[1].legend()\n",
        "axs[2].plot(i,IF_data[0],label=\"IF test data\",color=\"red\")\n",
        "axs[2].plot(i,IF_data[1],label=\"IF training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[2].plot(i,IF_data[2],label=\"IF unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[2].set_xlabel('Number of epochs')\n",
        "axs[2].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[2].set_yscale('log')\n",
        "axs[2].legend()\n",
        "fig.suptitle('Accuracy')"
      ],
      "metadata": {
        "id": "vEV1CkBcGRjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nQubit = 3\n",
        "layers = [40,40,2**nQubit]\n",
        "scale = 1 # scale the variance of the initial params\n",
        "num_epochs = 300\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "training_states = 3800\n",
        "test_states = 200\n",
        "training_data = np.zeros((training_states,2**nQubit*2))\n",
        "test_data = np.zeros((test_states,2**nQubit*2))\n",
        "theta = 0\n",
        "phi = 0\n",
        "p = 0.15\n",
        "\n",
        "model = MLP(num_neurons_per_layer=layers)        # Initialize network\n",
        "x_key, init_key = random.split(random.PRNGKey(0))\n",
        "x = scale*random.normal(x_key, (batch_size, 2**nQubit))                    # Batch size 100, input size 2\n",
        "params = model.init(init_key, x)                      # Initialize network params (biases being initialized as zeros)\n",
        "\n",
        "optimizer = optax.adam(learning_rate=learning_rate) #sgd\n",
        "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
        "                                            params=params,\n",
        "                                            tx=optimizer)\n",
        "\n",
        "POVM = generate_POVM(theta,phi,nQubit)\n",
        "\n",
        "for i in tqdm(range(training_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #training_data[i][j] = noisy_probs_sampled[j]\n",
        "        training_data[i][j] = noisy_probs[j]\n",
        "        training_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "for i in tqdm(range(test_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #test_data[i][j] = noisy_probs_sampled[j]\n",
        "        test_data[i][j] = noisy_probs[j]\n",
        "        test_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "data_loader_training = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_training_input = []  \n",
        "batched_training_target_outputs = []\n",
        "\n",
        "for batch in data_loader_training:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_training_input.append(inputs)\n",
        "  batched_training_target_outputs.append(targets)\n",
        "\n",
        "\n",
        "data_loader_test = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_test_input = []  \n",
        "batched_test_target_outputs = []\n",
        "\n",
        "for batch in data_loader_test:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_test_input.append(inputs)\n",
        "  batched_test_target_outputs.append(targets)\n",
        "\n",
        "batched_tr_inp = jnp.asarray(batched_training_input)\n",
        "batched_tr_tar_out = jnp.asarray(batched_training_target_outputs)\n",
        "batched_test_inp = jnp.asarray(batched_test_input)\n",
        "batched_test_tar_out = jnp.asarray(batched_test_target_outputs)\n",
        "\n",
        "trained_model_state3, MSE_data3, KLD_data3, IF_data3 = train_model(model_state, batched_tr_inp, batched_tr_tar_out, \n",
        "                                                               batched_test_inp, batched_test_tar_out, \n",
        "                                                               nQubit=nQubit, num_epochs = num_epochs)\n",
        "\n",
        "i = np.arange(0,num_epochs)\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "axs[0].plot(i,MSE_data3[0],label=\"MSE test data\",color=\"red\")\n",
        "axs[0].plot(i,MSE_data3[1],label=\"MSE training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[0].plot(i,MSE_data3[2],label=\"MSE unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[0].set_xlabel('Number of epochs')\n",
        "axs[0].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[0].set_yscale('log')\n",
        "axs[0].legend()\n",
        "axs[1].plot(i,KLD_data3[0],label=\"KLD test data\",color=\"red\")\n",
        "axs[1].plot(i,KLD_data3[1],label=\"KLD training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[1].plot(i,KLD_data3[2],label=\"KLD unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[1].set_xlabel('Number of epochs')\n",
        "axs[1].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[1].set_yscale('log')\n",
        "axs[1].legend()\n",
        "axs[2].plot(i,IF_data3[0],label=\"IF test data\",color=\"red\")\n",
        "axs[2].plot(i,IF_data3[1],label=\"IF training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[2].plot(i,IF_data3[2],label=\"IF unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[2].set_xlabel('Number of epochs')\n",
        "axs[2].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[2].set_yscale('log')\n",
        "axs[2].legend()\n",
        "fig.suptitle('Accuracy')"
      ],
      "metadata": {
        "id": "ZcLmxZNm4AhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nQubit = 4\n",
        "layers = [80,80,80,80,80,80,80,2**nQubit]\n",
        "scale = 1 # scale the variance of the initial params\n",
        "num_epochs = 300\n",
        "batch_size = 50\n",
        "learning_rate = 5*10**-5\n",
        "training_states = 7800\n",
        "test_states = 200\n",
        "training_data = np.zeros((training_states,2**nQubit*2))\n",
        "test_data = np.zeros((test_states,2**nQubit*2))\n",
        "theta = 0\n",
        "phi = 0\n",
        "p = 0.15\n",
        "\n",
        "model = MLP(num_neurons_per_layer=layers)        # Initialize network\n",
        "x_key, init_key = random.split(random.PRNGKey(0))\n",
        "x = scale*random.normal(x_key, (batch_size, 2**nQubit))                    # Batch size 100, input size 2\n",
        "params = model.init(init_key, x)                      # Initialize network params (biases being initialized as zeros)\n",
        "\n",
        "optimizer = optax.adam(learning_rate=learning_rate) #sgd\n",
        "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
        "                                            params=params,\n",
        "                                            tx=optimizer)\n",
        "\n",
        "POVM = generate_POVM(theta,phi,nQubit)\n",
        "\n",
        "for i in tqdm(range(training_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #training_data[i][j] = noisy_probs_sampled[j]\n",
        "        training_data[i][j] = noisy_probs[j]\n",
        "        training_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "for i in tqdm(range(test_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #test_data[i][j] = noisy_probs_sampled[j]\n",
        "        test_data[i][j] = noisy_probs[j]\n",
        "        test_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "data_loader_training = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_training_input = []  \n",
        "batched_training_target_outputs = []\n",
        "\n",
        "for batch in data_loader_training:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_training_input.append(inputs)\n",
        "  batched_training_target_outputs.append(targets)\n",
        "\n",
        "\n",
        "data_loader_test = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_test_input = []  \n",
        "batched_test_target_outputs = []\n",
        "\n",
        "for batch in data_loader_test:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_test_input.append(inputs)\n",
        "  batched_test_target_outputs.append(targets)\n",
        "\n",
        "batched_tr_inp = jnp.asarray(batched_training_input)\n",
        "batched_tr_tar_out = jnp.asarray(batched_training_target_outputs)\n",
        "batched_test_inp = jnp.asarray(batched_test_input)\n",
        "batched_test_tar_out = jnp.asarray(batched_test_target_outputs)\n",
        "\n",
        "trained_model_state4, MSE_data4, KLD_data4, IF_data4 = train_model(model_state, batched_tr_inp, batched_tr_tar_out, \n",
        "                                                               batched_test_inp, batched_test_tar_out, \n",
        "                                                               nQubit=nQubit, num_epochs = num_epochs)\n",
        "\n",
        "i = np.arange(0,num_epochs)\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "axs[0].plot(i,MSE_data4[0],label=\"MSE test data\",color=\"red\")\n",
        "axs[0].plot(i,MSE_data4[1],label=\"MSE training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[0].plot(i,MSE_data4[2],label=\"MSE unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[0].set_xlabel('Number of epochs')\n",
        "axs[0].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[0].set_yscale('log')\n",
        "axs[0].legend()\n",
        "axs[1].plot(i,KLD_data4[0],label=\"KLD test data\",color=\"red\")\n",
        "axs[1].plot(i,KLD_data4[1],label=\"KLD training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[1].plot(i,KLD_data4[2],label=\"KLD unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[1].set_xlabel('Number of epochs')\n",
        "axs[1].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[1].set_yscale('log')\n",
        "axs[1].legend()\n",
        "axs[2].plot(i,IF_data4[0],label=\"IF test data\",color=\"red\")\n",
        "axs[2].plot(i,IF_data4[1],label=\"IF training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[2].plot(i,IF_data4[2],label=\"IF unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[2].set_xlabel('Number of epochs')\n",
        "axs[2].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[2].set_yscale('log')\n",
        "axs[2].legend()\n",
        "fig.suptitle('Accuracy')"
      ],
      "metadata": {
        "id": "JXXwgEQs7Lr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nQubit = 5\n",
        "layers = [160,160,160,160,160,2**nQubit]\n",
        "scale = 1 # scale the variance of the initial params\n",
        "num_epochs = 300\n",
        "batch_size = 50\n",
        "learning_rate = 5*10**-5\n",
        "training_states = 9850\n",
        "test_states = 200\n",
        "training_data = np.zeros((training_states,2**nQubit*2))\n",
        "test_data = np.zeros((test_states,2**nQubit*2))\n",
        "theta = 0\n",
        "phi = 0\n",
        "p = 0.15\n",
        "\n",
        "model = MLP(num_neurons_per_layer=layers)        # Initialize network\n",
        "x_key, init_key = random.split(random.PRNGKey(0))\n",
        "x = scale*random.normal(x_key, (batch_size, 2**nQubit))                    # Batch size 100, input size 2\n",
        "params = model.init(init_key, x)                      # Initialize network params (biases being initialized as zeros)\n",
        "\n",
        "optimizer = optax.adam(learning_rate=learning_rate) #sgd\n",
        "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
        "                                            params=params,\n",
        "                                            tx=optimizer)\n",
        "\n",
        "POVM = generate_POVM(theta,phi,nQubit)\n",
        "\n",
        "for i in tqdm(range(training_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #training_data[i][j] = noisy_probs_sampled[j]\n",
        "        training_data[i][j] = noisy_probs[j]\n",
        "        training_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "for i in tqdm(range(test_states)):\n",
        "    state = generate_random_seperable_pure_state(nQubit)\n",
        "    probs = perform_multi_qubit_measurement(state,POVM)\n",
        "    noisy_probs = perform_noisy_multi_qubit_measurement(state,POVM,p)\n",
        "    #noisy_probs_sampled = sampling(noisy_probs)\n",
        "    for j in range(2**nQubit):\n",
        "        #test_data[i][j] = noisy_probs_sampled[j]\n",
        "        test_data[i][j] = noisy_probs[j]\n",
        "        test_data[i][j+2**nQubit] = probs[j]\n",
        "\n",
        "data_loader_training = data.DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_training_input = []  \n",
        "batched_training_target_outputs = []\n",
        "\n",
        "for batch in data_loader_training:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_training_input.append(inputs)\n",
        "  batched_training_target_outputs.append(targets)\n",
        "\n",
        "\n",
        "data_loader_test = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate)\n",
        "batched_test_input = []  \n",
        "batched_test_target_outputs = []\n",
        "\n",
        "for batch in data_loader_test:\n",
        "  inputs, targets = jnp.split(batch,2,axis=1)\n",
        "  batched_test_input.append(inputs)\n",
        "  batched_test_target_outputs.append(targets)\n",
        "\n",
        "batched_tr_inp = jnp.asarray(batched_training_input)\n",
        "batched_tr_tar_out = jnp.asarray(batched_training_target_outputs)\n",
        "batched_test_inp = jnp.asarray(batched_test_input)\n",
        "batched_test_tar_out = jnp.asarray(batched_test_target_outputs)\n",
        "\n",
        "trained_model_state5, MSE_data5, KLD_data5, IF_data5 = train_model(model_state, batched_tr_inp, batched_tr_tar_out, \n",
        "                                                               batched_test_inp, batched_test_tar_out, \n",
        "                                                               nQubit=nQubit, num_epochs = num_epochs)\n",
        "\n",
        "i = np.arange(0,num_epochs)\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "axs[0].plot(i,MSE_data5[0],label=\"MSE test data\",color=\"red\")\n",
        "axs[0].plot(i,MSE_data5[1],label=\"MSE training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[0].plot(i,MSE_data5[2],label=\"MSE unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[0].set_xlabel('Number of epochs')\n",
        "axs[0].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[0].set_yscale('log')\n",
        "axs[0].legend()\n",
        "axs[1].plot(i,KLD_data5[0],label=\"KLD test data\",color=\"red\")\n",
        "axs[1].plot(i,KLD_data5[1],label=\"KLD training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[1].plot(i,KLD_data5[2],label=\"KLD unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[1].set_xlabel('Number of epochs')\n",
        "axs[1].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[1].set_yscale('log')\n",
        "axs[1].legend()\n",
        "axs[2].plot(i,IF_data5[0],label=\"IF test data\",color=\"red\")\n",
        "axs[2].plot(i,IF_data5[1],label=\"IF training data\",linestyle=\"--\",color=\"black\")\n",
        "axs[2].plot(i,IF_data5[2],label=\"IF unmitigated test data\",linestyle=\"-.\",color=\"blue\")\n",
        "axs[2].set_xlabel('Number of epochs')\n",
        "axs[2].set_ylabel('Accuracy / Mean loss over all datapoints')\n",
        "axs[2].set_yscale('log')\n",
        "axs[2].legend()\n",
        "fig.suptitle('Accuracy')"
      ],
      "metadata": {
        "id": "lFW68fbV7MRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"2 Qubits\",\"3 Qubits\",\"4 Qubits\",\"5 Qubits\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=False)\n",
        "\n",
        "axs[0].bar(0,MSE_data[2][-1],label=\"unmitigated test data\",color=\"black\")\n",
        "axs[0].bar(1,MSE_data[1][-1],label=\"training data\",color=\"blue\")\n",
        "axs[0].bar(2,MSE_data[0][-1],label=\"test data\",color=\"red\")\n",
        "axs[0].bar(5,MSE_data3[2][-1],color=\"black\")\n",
        "axs[0].bar(6,MSE_data3[1][-1],color=\"blue\")\n",
        "axs[0].bar(7,MSE_data3[0][-1],color=\"red\")\n",
        "axs[0].bar(10,MSE_data4[2][-1],color=\"black\")\n",
        "axs[0].bar(11,MSE_data4[1][-1],color=\"blue\")\n",
        "axs[0].bar(12,MSE_data4[0][-1],color=\"red\")\n",
        "axs[0].bar(15,MSE_data5[2][-1],color=\"black\")\n",
        "axs[0].bar(16,MSE_data5[1][-1],color=\"blue\")\n",
        "axs[0].bar(17,MSE_data5[0][-1],color=\"red\")\n",
        "axs[0].set_xticks([1,5,11,16])\n",
        "axs[0].set_xticklabels(labels)\n",
        "axs[0].set_xlabel(\"MSE\")\n",
        "axs[0].set_ylabel(\"accuracy\")\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].bar(0,KLD_data[2][-1],label=\"unmitigated test data\",color=\"black\")\n",
        "axs[1].bar(1,KLD_data[1][-1],label=\"training data\",color=\"blue\")\n",
        "axs[1].bar(2,KLD_data[0][-1],label=\"test data\",color=\"red\")\n",
        "axs[1].bar(5,KLD_data3[2][-1],color=\"black\")\n",
        "axs[1].bar(6,KLD_data3[1][-1],color=\"blue\")\n",
        "axs[1].bar(7,KLD_data3[0][-1],color=\"red\")\n",
        "axs[1].bar(10,KLD_data4[2][-1],color=\"black\")\n",
        "axs[1].bar(11,KLD_data4[1][-1],color=\"blue\")\n",
        "axs[1].bar(12,KLD_data4[0][-1],color=\"red\")\n",
        "axs[1].bar(15,KLD_data5[2][-1],color=\"black\")\n",
        "axs[1].bar(16,KLD_data5[1][-1],color=\"blue\")\n",
        "axs[1].bar(17,KLD_data5[0][-1],color=\"red\")\n",
        "axs[1].set_xticks([1,5,11,16])\n",
        "axs[1].set_xticklabels(labels)\n",
        "axs[1].set_xlabel(\"KLD\")\n",
        "axs[1].set_ylabel(\"accuracy\")\n",
        "axs[1].legend()\n",
        "\n",
        "axs[2].bar(0,IF_data[2][-1],label=\"unmitigated test data\",color=\"black\")\n",
        "axs[2].bar(1,IF_data[1][-1],label=\"training data\",color=\"blue\")\n",
        "axs[2].bar(2,IF_data[0][-1],label=\"test data\",color=\"red\")\n",
        "axs[2].bar(5,IF_data3[2][-1],color=\"black\")\n",
        "axs[2].bar(6,IF_data3[1][-1],color=\"blue\")\n",
        "axs[2].bar(7,IF_data3[0][-1],color=\"red\")\n",
        "axs[2].bar(10,IF_data4[2][-1],color=\"black\")\n",
        "axs[2].bar(11,IF_data4[1][-1],color=\"blue\")\n",
        "axs[2].bar(12,IF_data4[0][-1],color=\"red\")\n",
        "axs[2].bar(15,IF_data5[2][-1],color=\"black\")\n",
        "axs[2].bar(16,IF_data5[1][-1],color=\"blue\")\n",
        "axs[2].bar(17,IF_data5[0][-1],color=\"red\")\n",
        "axs[2].set_xticks([1,5,11,16])\n",
        "axs[2].set_xticklabels(labels)\n",
        "axs[2].set_xlabel(\"IF\")\n",
        "axs[2].set_ylabel(\"accuracy\")\n",
        "axs[2].legend()\n",
        "\n",
        "fig.suptitle('NN improvement')"
      ],
      "metadata": {
        "id": "MYcOchLS8apJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}